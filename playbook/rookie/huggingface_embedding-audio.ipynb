{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embeddings with the transformer library\n",
    "\n",
    "We use the Huggingface transformers library to create an embedding for a an image dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tldr; Play as callable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoFeatureExtractor, AutoModel, ASTForAudioClassification\n",
    "import torch\n",
    "from renumics import spotlight\n",
    "import pandas as pd\n",
    "import umap\n",
    "import numpy as np\n",
    "\n",
    "def __set_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   \n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    return device\n",
    "\n",
    "\n",
    "def extract_embeddings(model, feature_extractor):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        audios = [element[\"array\"] for element in batch[\"audio\"]]\n",
    "        inputs = feature_extractor(raw_speech=audios, return_tensors=\"pt\", padding=True).to(device)        \n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0].cpu()\n",
    "        \n",
    "        return {\"embedding\": embeddings}\n",
    "        \n",
    "\n",
    "    return pp\n",
    "\n",
    "\n",
    "def huggingface_embedding(dataset, modelname, batched=True, batch_size=8):\n",
    "    # initialize huggingface model\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(modelname, padding=True)\n",
    "    model = AutoModel.from_pretrained(modelname, output_hidden_states=True)\n",
    "\n",
    "    #compute embedding  \n",
    "    device = __set_device()\n",
    "    extract_fn = extract_embeddings(model.to(device), feature_extractor)\n",
    "    updated_dataset = dataset.map(extract_fn, batched=batched, batch_size=batch_size, cache_file_name=\"updated_embedding\")\n",
    "    \n",
    "    return updated_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step example on speech-commands\n",
    "\n",
    "### Load speech-commands from Huggingface hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use test split to evaluate model's performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('speech_commands', 'v0.01', split=\"all\")\n",
    "labels = dataset.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at all of the labels that we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embedding with audio transformer from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_enriched = huggingface_embedding(dataset, \"MIT/ast-finetuned-speech-commands-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce embeddings for faster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.stack(np.array(dataset_enriched['embedding']))\n",
    "reducer = umap.UMAP()\n",
    "reduced_embedding = reducer.fit_transform(embeddings)\n",
    "dataset_enriched = dataset_enriched.add_column(\"embedding_reduced\", list(reduced_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform EDA with Spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotlight.show(df, dtype={\"audio\": spotlight.Audio, \"embedding\": spotlight.Embedding, \n",
    "                                   \"embedding_reduced\": spotlight.Embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save enriched dataframe to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_enriched.to_parquet('dataset_audio_annotated_and_embedding.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0712b5d67739eb35ad63a27edd5fddd52f439fecfdecbb3b06a7c473af2eb31"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
