{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image embeddings with Huggingface\n",
    "\n",
    "We use the [Huggingface transformers library](https://github.com/huggingface/transformers) to create an embedding for a an image dataset. \n",
    "\n",
    "More information about this play can be found in the Spotlight documentation: [Create image embeddings with the Huggingface transformer library](https://renumics.com/docs/playbook/huggingface-embedding)\n",
    "\n",
    "For more data-centric AI workflows, check out our [Awesome Open Data-centric AI](https://github.com/Renumics/awesome-open-data-centric-ai) list on Github.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install required packages with PIP\n",
    "\n",
    "!pip install renumics-spotlight transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Play as copy-n-paste functions\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "import torch\n",
    "from renumics import spotlight\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_embeddings(model, feature_extractor, image_name='image'):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]        \n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)        \n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0].cpu()\n",
    "        \n",
    "        return {\"embedding\": embeddings}\n",
    "        \n",
    "\n",
    "    return pp\n",
    "\n",
    "\n",
    "def huggingface_embedding(df, image_name='image', inplace=False, modelname='google/vit-base-patch16-224', batched=True, batch_size=24):\n",
    "    # initialize huggingface model\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(modelname)\n",
    "    model = AutoModel.from_pretrained(modelname, output_hidden_states=True)\n",
    "\n",
    "    # create huggingface dataset from df\n",
    "    dataset = datasets.Dataset.from_pandas(df).cast_column(image_name, datasets.Image())\n",
    "\n",
    "    #compute embedding  \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"    \n",
    "    extract_fn = extract_embeddings( model.to(device), feature_extractor,image_name)\n",
    "    updated_dataset = dataset.map(extract_fn, batched=batched, batch_size=batch_size)\n",
    "    \n",
    "    df_temp = updated_dataset.to_pandas()\n",
    "\n",
    "    if inplace:\n",
    "        df['embedding']=df_temp['embedding']\n",
    "        return\n",
    "\n",
    "    df_emb = pd.DataFrame()\n",
    "    df_emb['embedding'] = df_temp['embedding']\n",
    "\n",
    "    \n",
    "\n",
    "    return df_emb\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step example on CIFAR-100\n",
    "\n",
    "### Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"renumics/cifar100-enriched\", split=\"train\")\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embedding with vision transformer from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb = huggingface_embedding(df, modelname=\"google/vit-base-patch16-224\")\n",
    "df = pd.concat([df, df_emb], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce embeddings for faster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "embeddings = np.stack(df['embedding'].to_numpy())\n",
    "reducer = umap.UMAP()\n",
    "reduced_embedding = reducer.fit_transform(embeddings)\n",
    "df['embedding_reduced'] = np.array(reduced_embedding).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform EDA with Spotlight\n",
    "\n",
    "> ⚠️ Running Spotlight in Colab currently has severe limitations (slow, no similarity map, no layouts) due to Colab restrictions (e.g. no websocket support). Run the notebook locally for the full Spotlight experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show = df.drop(columns=['embedding', 'probabilities'])\n",
    "\n",
    "# handle google colab differently\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    #visualize subset in Google Colab\n",
    "    port=50123\n",
    "    spotlight.show(df_show[:10000], port=port, dtype={\"image\": spotlight.Image, \"embedding_reduced\": spotlight.Embedding})\n",
    "  \n",
    "    from google.colab.output import eval_js  # type: ignore\n",
    "    print(str(eval_js(f\"google.colab.kernel.proxyPort({port}, {{'cache': true}})\")))\n",
    "\n",
    "else:\n",
    "    spotlight.show(df_show, dtype={\"image\": spotlight.Image, \"embedding_reduced\": spotlight.Embedding})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
